{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URDU NLP SPACY TOKENIZER\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('ur')\n",
    "\n",
    "def urdu_tokens(inp):\n",
    "   # print(\"The inp to urdu tokens is :\",inp)\n",
    "    doc=nlp(inp)\n",
    "    lst=[]\n",
    "    lst.clear()\n",
    "    for word in doc:\n",
    "        word=str(word)\n",
    "        lst.append(word)\n",
    "\n",
    "    return lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('ur')\n",
    "\n",
    "def urdu_sen_tokens(inp):\n",
    "   # print(\"The inp to urdu tokens is :\",inp)\n",
    "    doc=nlp(inp)\n",
    "    doc=str(doc)\n",
    "    doc=doc.replace(\"\\n\",\"\")\n",
    "    return [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "def eng_sen_tokens(inp):\n",
    "   # print(\"The inp to eng tokens is :\",inp)\n",
    "    doc=nlp(inp)\n",
    "    doc=str(doc)\n",
    "    #print(\"doc type is :\",type(doc))\n",
    "    \n",
    "    #print(\"ret type is \",type([doc]))\n",
    "    doc=doc.replace(\"\\n\",\"\")\n",
    "    return [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "\n",
    "#d_model=size of input embeddings#just a vec represenation of the particular word\n",
    "\n",
    "\n",
    "\n",
    "#feed forward = bunch of linear layers where relu is in between or any other activ function(convolution layer)\n",
    "#n_head =the number of heads in the multiheadattention models\n",
    "#number of encoder decoder layers stacked up with each other\n",
    "#dim_feedforward=Matrix of feedforward size\n",
    "#dropout is applied on hidden and input layers to basically stop some neurons from working to handle overfitting\n",
    "\n",
    "#Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "#layer norm is applied in transformer and ep value represents the value that is added in denominator of normalization for stability \n",
    "#if batch first is true then  input and output tensors are provided  as (batch, seq, feature).  else (seq,batch,feature)\n",
    "#if ``True``, encoder and decoder layers will perform LayerNorms before other attention and feedforward operations, otherwise after. Default: ``False`\n",
    "class Transformer(Module):\n",
    "   \n",
    "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        if custom_encoder is not None:\n",
    "            self.encoder = custom_encoder\n",
    "        else:\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                    activation, layer_norm_eps, batch_first, norm_first,\n",
    "                                                    **factory_kwargs)\n",
    "            encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        if custom_decoder is not None:\n",
    "            self.decoder = custom_decoder\n",
    "        else:\n",
    "            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                    activation, layer_norm_eps, batch_first, norm_first,\n",
    "                                                    **factory_kwargs)\n",
    "            decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        #just init , make encoder ,decoder,norm,layers\n",
    "        \n",
    " #logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    " #                             src_padding_mask, tgt_padding_mask, src_padding_mask)        \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \n",
    "        is_batched = src.dim() == 3\n",
    "        if not self.batch_first and src.size(1) != tgt.size(1) and is_batched:\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "        elif self.batch_first and src.size(0) != tgt.size(0) and is_batched:\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "\n",
    "        if src.size(-1) != self.d_model or tgt.size(-1) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)#encoding forward pass\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)#decoding forward pass\n",
    "        return output#the forward pass of decoder is our output\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "    #Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "    #        Unmasked positions are filled with float(0.0).\n",
    "    #return that masked array\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)#function of torch for xavier initiliaztion\n",
    "\n",
    "\n",
    "\n",
    "#TransformerEncoder is a stack of N encoder layers\n",
    "class TransformerEncoder(Module):\n",
    "\n",
    "    __constants__ = ['norm']\n",
    "    #encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "#        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "#        norm: the layer normalization component (optional).\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=True):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.enable_nested_tensor = enable_nested_tensor\n",
    "\n",
    "#Pass the input through the encoder layers in turn.\n",
    "#src: the sequence to the encoder (required).\n",
    "#mask: the mask for the src sequence (optional).\n",
    "#src mask=is to do -inf\n",
    "#tgt mask=0\n",
    "#memory mask= -inf to some mask\n",
    "#src_key_padd_mask the ByteTensor mask for src keys per batch (optional). Since your src usually has different lengths sequences it's common to remove the padding vectors you appended at the end. For this you specify the length of each sequence per example in your batch. \n",
    "#src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "#in this we just have to run the forward passs of encoder layer\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = src\n",
    "        convert_to_nested = False\n",
    "        first_layer = self.layers[0]\n",
    "        if isinstance(first_layer, torch.nn.TransformerEncoderLayer):\n",
    "            if (not first_layer.norm_first and not first_layer.training and\n",
    "                    first_layer.self_attn.batch_first and\n",
    "                    first_layer.self_attn._qkv_same_embed_dim and first_layer.activation_relu_or_gelu and\n",
    "                    first_layer.norm1.eps == first_layer.norm2.eps and\n",
    "                    src.dim() == 3 and self.enable_nested_tensor) :\n",
    "                if src_key_padding_mask is not None and not output.is_nested and mask is None:\n",
    "                    tensor_args = (\n",
    "                        src,\n",
    "                        first_layer.self_attn.in_proj_weight,\n",
    "                        first_layer.self_attn.in_proj_bias,\n",
    "                        first_layer.self_attn.out_proj.weight,\n",
    "                        first_layer.self_attn.out_proj.bias,\n",
    "                        first_layer.norm1.weight,\n",
    "                        first_layer.norm1.bias,\n",
    "                        first_layer.norm2.weight,\n",
    "                        first_layer.norm2.bias,\n",
    "                        first_layer.linear1.weight,\n",
    "                        first_layer.linear1.bias,\n",
    "                        first_layer.linear2.weight,\n",
    "                        first_layer.linear2.bias,\n",
    "                    )\n",
    "                    if not torch.overrides.has_torch_function(tensor_args):\n",
    "                        if output.is_cuda or 'cpu' in str(output.device):\n",
    "                            convert_to_nested = True\n",
    "                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\n",
    "\n",
    "        for mod in self.layers:\n",
    "            if convert_to_nested:\n",
    "                output = mod(output, src_mask=mask)\n",
    "            else:\n",
    "                output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if convert_to_nested:\n",
    "            output = output.to_padded_tensor(0.)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "#TransformerDecoder is a stack of N Decoder layers\n",
    "class TransformerDecoder(Module):\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "#TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "#d_model=size of input embeddings#just a vec represenation of the particular word\n",
    "\n",
    "\n",
    "\n",
    "#feed forward = bunch of linear layers where relu is in between or any other activ function(convolution layer)\n",
    "#n_head =the number of heads in the multiheadattention models\n",
    "#dim_feedforward=Matrix of feedforward size\n",
    "#dropout is applied on hidden and input layers to basically stop some neurons from working to handle overfitting\n",
    "\n",
    "#Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "#layer norm is applied in transformer and ep value represents the value that is added in denominator of normalization for stability \n",
    "\n",
    "#if batch first is true then  input and output tensors are provided  as (batch, seq, feature).  else (seq,batch,feature)\n",
    "#if ``True``, encoder and decoder layers will perform LayerNorms before other attention and feedforward operations, otherwise after. Default: ``False`\n",
    "\n",
    "class TransformerEncoderLayer(Module):\n",
    "  \n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)#input features,output features\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            activation = _get_activation_fn(activation)\n",
    "\n",
    "        if activation is F.relu:\n",
    "            self.activation_relu_or_gelu = 1\n",
    "        elif activation is F.gelu:\n",
    "            self.activation_relu_or_gelu = 2\n",
    "        else:\n",
    "            self.activation_relu_or_gelu = 0\n",
    "        self.activation = activation\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        if (src.dim() == 3 and not self.norm_first and not self.training and\n",
    "            self.self_attn.batch_first and\n",
    "            self.self_attn._qkv_same_embed_dim and self.activation_relu_or_gelu and\n",
    "            self.norm1.eps == self.norm2.eps and\n",
    "            ((src_mask is None and src_key_padding_mask is None)\n",
    "             if src.is_nested\n",
    "             else (src_mask is None or src_key_padding_mask is None))):\n",
    "            tensor_args = (\n",
    "                src,\n",
    "                self.self_attn.in_proj_weight,\n",
    "                self.self_attn.in_proj_bias,\n",
    "                self.self_attn.out_proj.weight,\n",
    "                self.self_attn.out_proj.bias,\n",
    "                self.norm1.weight,\n",
    "                self.norm1.bias,\n",
    "                self.norm2.weight,\n",
    "                self.norm2.bias,\n",
    "                self.linear1.weight,\n",
    "                self.linear1.bias,\n",
    "                self.linear2.weight,\n",
    "                self.linear2.bias,\n",
    "            )##biases and weights\n",
    "            if (not torch.overrides.has_torch_function(tensor_args) and\n",
    "                    # We have to use a list comprehension here because TorchScript\n",
    "                    # doesn't support generator expressions.\n",
    "                    all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]) and\n",
    "                    (not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]))):\n",
    "                return torch._transformer_encoder_layer_fwd(\n",
    "                    src,\n",
    "                    self.self_attn.embed_dim,\n",
    "                    self.self_attn.num_heads,\n",
    "                    self.self_attn.in_proj_weight,\n",
    "                    self.self_attn.in_proj_bias,\n",
    "                    self.self_attn.out_proj.weight,\n",
    "                    self.self_attn.out_proj.bias,\n",
    "                    self.activation_relu_or_gelu == 2,\n",
    "                    False,  # norm_first, currently not supported\n",
    "                    self.norm1.eps,\n",
    "                    self.norm1.weight,\n",
    "                    self.norm1.bias,\n",
    "                    self.norm2.weight,\n",
    "                    self.norm2.bias,\n",
    "                    self.linear1.weight,\n",
    "                    self.linear1.bias,\n",
    "                    self.linear2.weight,\n",
    "                    self.linear2.bias,\n",
    "                    src_mask if src_mask is not None else src_key_padding_mask,\n",
    "                )\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)\n",
    "            x = x + self._ff_block(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))\n",
    "            x = self.norm2(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(Module):\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                                 **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.dropout3 = Dropout(dropout)\n",
    "\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = _get_activation_fn(activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
    "            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)\n",
    "            x = x + self._ff_block(self.norm3(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))\n",
    "            x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # multihead attention block\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.multihead_attn(x, mem, mem,\n",
    "                                attn_mask=attn_mask,\n",
    "                                key_padding_mask=key_padding_mask,\n",
    "                                need_weights=False)[0]\n",
    "        return self.dropout2(x)\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation: str) -> Callable[[Tensor], Tensor]:\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torch import Tensor\n",
    "import io\n",
    "import time\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "fileEnglish = open('English.txt', mode='rt', encoding='utf-8')\n",
    "englishDataset = fileEnglish.read()\n",
    "\n",
    "file = open(\"English.txt\", \"r\",encoding='utf-8')\n",
    "x = 0\n",
    "for line in file:\n",
    "\n",
    "    if line != \"\\n\":\n",
    "        x += 1\n",
    "file.close()\n",
    "\n",
    "\n",
    "fileUrdu = open('Urdu.txt', mode='rt', encoding='utf-8')\n",
    "urduDataset = fileUrdu.read()\n",
    "\n",
    "\n",
    "train_size_en = int(0.70 * x)\n",
    "test_size_en = int(0.15 * x)\n",
    "val_size_en = int(0.15 * x)#division on the basis of lines\n",
    "\n",
    "\n",
    "train_dataset=englishDataset[0:train_size_en]\n",
    "test_dataset=englishDataset[train_size_en+1:train_size_en+test_size_en]\n",
    "val_dataset=englishDataset[train_size_en+test_size_en+1:x]\n",
    "\n",
    "file = open(\"Urdu.txt\", \"r\",encoding='utf-8')\n",
    "x = 0\n",
    "for line in file:\n",
    "\n",
    "    if line != \"\\n\":\n",
    "        x += 1\n",
    "file.close()\n",
    "\n",
    "train_size_urdu= int(0.70 * x)\n",
    "test_size_urdu = int(0.15 *x)\n",
    "val_size_urdu = int(0.15 * x)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset2=urduDataset[0:train_size_urdu]\n",
    "test_dataset2=urduDataset[train_size_urdu+1:train_size_urdu+test_size_urdu]\n",
    "val_dataset2=urduDataset[train_size_urdu+test_size_urdu+1:x]\n",
    "#train_dataset, test_dataset,val_dataset = torch.utils.data.random_split(englishDataset, [train_size_en, test_size_en,val_size_en])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f=open(\"english_train.txt\",\"w\")\n",
    "\n",
    "f.write(train_dataset)\n",
    "\n",
    "f=open(\"english_test.txt\",\"w\")\n",
    "f.write(test_dataset)\n",
    "\n",
    "f=open(\"english_val.txt\",\"w\")\n",
    "f.write(val_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f=open(\"urdu_train.txt\",\"w\",encoding=\"utf-8\")\n",
    "\n",
    "f.write(train_dataset2)\n",
    "\n",
    "f=open(\"urdu_test.txt\",\"w\",encoding=\"utf-8\")\n",
    "f.write(test_dataset2)\n",
    "\n",
    "f=open(\"urdu_val.txt\",\"w\",encoding=\"utf-8\")\n",
    "f.write(val_dataset2)\n",
    "\n",
    "de_tokenizer = urdu_sen_tokens\n",
    "en_tokenizer = eng_sen_tokens#get_tokenizer('spacy', language='en_core_web_sm')#so the tokenizer here points to the function of tokenizer\n",
    "#which then gets passed to build vocab function \n",
    "#and in the tokenizer we pass the line\n",
    "\n",
    "\n",
    "#Building voabulary on the basis of its frequency\n",
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()#counter builds dictionary of word with its frequencies\n",
    "  with io.open(filepath,encoding=\"utf8\",errors='ignore' ) as f:\n",
    "    for string_ in f:\n",
    "      #print(tokenizer(string_))\n",
    "      counter.update(tokenizer(string_))\n",
    "  return vocab(counter,specials = ['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "main_filepaths=[\"English.txt\",\"Urdu.txt\"]\n",
    "train_filepaths = [\"english_train.txt\",\"urdu_train.txt\"]\n",
    "val_filepaths = [\"english_val.txt\",\"urdu_val.txt\"]\n",
    "test_filepaths = [\"english_test.txt\",\"urdu_test.txt\"]\n",
    "en_vocab = build_vocab(train_filepaths[0], en_tokenizer)\n",
    "de_vocab = build_vocab(train_filepaths[1], de_tokenizer)\n",
    "\n",
    "\n",
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[1], encoding=\"utf8\",errors='ignore'))#created iterator for urdu file\n",
    "  raw_en_iter = iter(io.open(filepaths[0], encoding=\"utf8\",errors='ignore'))#created english iterator for english file\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):#use zip to make alignment between files\n",
    "        #such as Los angeles with los angeles\n",
    "        #raw_de ,raw_en is for accessing that tuple(las angeles in urdu,los angeles in englus)\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de.rstrip(\"\\n\"))],\n",
    "                            dtype=torch.long)\n",
    "    #full urdu sentence with \\n where the eol character is remover using de_strip is tokenized and each token is search in the vocabulary\n",
    "    #whereas the voabulary is containing the embedding on the basis of frequency\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"\\n\"))],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "de_vocab.set_default_index(0)\n",
    "en_vocab.set_default_index(0)#if no index is found then 0 is returned\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#In case you didn’t know, tensors are matrices that can be stored in a GPU, and since they are matrices, all dimensions must have elements of the same size. Of course, this won’t happen when treating with tasks like NLP or different-sized images. Therefore, we use the so-called “special tokens”. These tokens allow our model to know where the start of the sentence is (<SOS>), where the end of the sentence is (<EOS>) and what elements are just there to fill up the remaining space so that our matrices have the sam sequence size (<PAD>). These tokens must also be converted into their corresponding integer id (In our example they will be 2, 3, and 4 respectively). Padding a sequence looks something like this:\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']#padding in sentence\n",
    "BOS_IDX = de_vocab['<bos>']#beggining of sentence\n",
    "EOS_IDX = de_vocab['<eos>']#representing end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "#basically concatenatnation start , end, pad and the tensory to make a element and append it in the batch\n",
    "def generate_batch(data_batch):\n",
    "  de_batch, en_batch = [], []\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))#torch.cat concatenates the tensors\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "  return de_batch, en_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)#dividing the data in batches with the batch size specified\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
    "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
    "                                        tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer_encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer_decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text tokens are represented by using token embeddings. Positional\n",
    "encoding is added to the token embedding to introduce a notion of word\n",
    "order.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q k lstm word by word leta he so usko yad rehta he k knsa  word pehle aya tha \n",
    "#on the other hand transformer ne q k poora sentence ek bar me lelena he so therefore it needs someway to remember which word comes in which order\n",
    "#but the order matters alot suppose not word if it comes he won he was not satisfied vs he not won but he was satisfied\n",
    "#toh iska solution he positional embeddings\n",
    "\n",
    "#so we will add the positon embedding in the actual word embeddings \n",
    "#but the values of position embedding will be?\n",
    "\n",
    "#they will be PE(pos,i)=sin(pos/10000**2i/d)\n",
    "# i is the index of the word \n",
    "#and pos is the position\n",
    "#where d=size of embeddings\n",
    "#pos 0 means the first positional embedding\n",
    "#pos 1 means the 2nd and so on\n",
    "#and i is the index in position embedding we are filling\n",
    "#pos 0 will have lower wavelength\n",
    "#and the higher the pos the more will be frequecny\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + \n",
    "                            self.pos_embedding[:token_embedding.size(0),:])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a ``subsequent word`` mask to stop a target word from\n",
    "attending to its subsequent words. We also create masks, for masking\n",
    "source and target padding tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "  src_seq_len = src.shape[0]\n",
    "  tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "  src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "  src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "  tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model parameters and instantiate model \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(de_vocab)\n",
    "TGT_VOCAB_SIZE = len(en_vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 16\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
    "                                 FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, optimizer):\n",
    "  model.train()\n",
    "  losses = 0\n",
    "  for idx, (src, tgt) in enumerate(train_iter):\n",
    "\n",
    "      src = src.to(device)\n",
    "      tgt = tgt.to(device)\n",
    "            \n",
    "      tgt_input = tgt[:-1, :]\n",
    "\n",
    "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "      logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      tgt_out = tgt[1:,:]\n",
    "      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      losses += loss.item()\n",
    "  return losses / len(train_iter)\n",
    "\n",
    "#cross entropy loss with masking and unmasking?\n",
    "#for mask filter we make a matrix of target_size^target_size\n",
    "#and pass 0 and negative inf\n",
    "#add it with attention filter to make mask-attention filter\n",
    "def evaluate(model, val_iter):\n",
    "  model.eval()\n",
    "  losses = 0\n",
    "  for idx, (src, tgt) in (enumerate(valid_iter)):\n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "\n",
    "    tgt_input = tgt[:-1, :]\n",
    "\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "    logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "    tgt_out = tgt[1:,:]\n",
    "    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "    losses += loss.item()\n",
    "  return losses / len(val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 1, Train loss: 4.954, Val loss: 4.020, Epoch time = 4.875s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 2, Train loss: 4.205, Val loss: 3.910, Epoch time = 4.797s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 3, Train loss: 3.941, Val loss: 4.154, Epoch time = 4.963s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 4, Train loss: 3.906, Val loss: 4.381, Epoch time = 5.814s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 5, Train loss: 3.882, Val loss: 4.513, Epoch time = 5.823s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 6, Train loss: 3.872, Val loss: 4.575, Epoch time = 5.626s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 7, Train loss: 3.838, Val loss: 4.789, Epoch time = 5.609s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 8, Train loss: 3.762, Val loss: 4.579, Epoch time = 5.456s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 9, Train loss: 3.614, Val loss: 4.423, Epoch time = 5.457s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 10, Train loss: 3.447, Val loss: 4.263, Epoch time = 5.557s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 11, Train loss: 3.254, Val loss: 4.376, Epoch time = 5.476s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 12, Train loss: 3.083, Val loss: 4.298, Epoch time = 5.476s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 13, Train loss: 2.928, Val loss: 4.139, Epoch time = 5.488s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 14, Train loss: 2.797, Val loss: 4.280, Epoch time = 5.474s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 15, Train loss: 2.677, Val loss: 4.155, Epoch time = 5.321s\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182ABD93460>\n",
      "Epoch: 16, Train loss: 2.547, Val loss: 4.109, Epoch time = 5.340s\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "  start_time = time.time()\n",
    "  train_loss = train_epoch(transformer, train_iter, optimizer)\n",
    "  end_time = time.time()\n",
    "  val_loss = evaluate(transformer, valid_iter)\n",
    "\n",
    "  print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "          f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    #in each iteration calculate bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(device)\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                                    .type(torch.bool)).to(device)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "          break\n",
    "    return ys\n",
    "\n",
    "#masking is to hide \n",
    "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
    "  model.eval()\n",
    "  tokens = [BOS_IDX] + [src_vocab.get_stoi()[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n",
    "  num_tokens = len(tokens)\n",
    "  src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
    "  src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "  tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "  return \" \".join([tgt_vocab.get_itos()[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Los Angeles has lost night straight and 13 of its first 14 games to start the season. '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(transformer, \"لاس اینجلس نے سیزن شروع کرنے کے لئے سیدھے رات اور اپنے پہلے 14 میں سے 13 کھیل کھوئے ہیں۔\", de_vocab, en_vocab, de_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Opposite qualities of meaning of person's name \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(transformer, \"آنکھ کا اندھا نام نین سکھ\", de_vocab, en_vocab, de_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To talk big without having a big position '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(transformer, \"چھوٹا منہ بڑی بات\", de_vocab, en_vocab, de_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لاس اینجلس نے سیزن شروع کرنے کے لئے سیدھے رات اور اپنے پہلے 14 میں سے 13 کھیل کھوئے ہیں۔\n",
      " Los Angeles has lost night straight and 13 of its first 14 games to start the season. \n",
      "آنکھ کا اندھا نام نین سکھ\n",
      " Opposite qualities of meaning of person's name \n",
      "کھسیانی بلی کھمبا نوچے\n",
      " To show anger after getting embarrassed \n",
      "چوری کا مال موری میں\n",
      " Money earned the wrong way will be taken away \n",
      "چھوٹا منہ بڑی بات\n",
      " To talk big without having a big position \n",
      "جتنے منہ اتنی باتیں\n",
      " More mouths will have more talks \n",
      "بہتی گنگا میں ہاتھ دھونا\n",
      " To use the available opportunity \n",
      "مان نہ مان میں تیرا مہمان\n",
      " Getting involved without having \n",
      "دور کے ڈھول سُہانے\n",
      " The grass is always greener on the other side \n",
      "گنگا گائے گنگا داس جمنا گائے جمنا داس\n",
      " A person of no principles \n"
     ]
    }
   ],
   "source": [
    "# Using readlines()\n",
    "file1 = open('urdu_train.txt', 'r',encoding='utf8')\n",
    "Lines = file1.readlines()\n",
    "  \n",
    "file2=open('trans.txt','w',encoding='utf8')\n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    count += 1\n",
    "    print(line.strip())\n",
    "    var=translate(transformer, line.strip(), de_vocab, en_vocab, de_tokenizer)\n",
    "    print(var)\n",
    "    file2.writelines(var)\n",
    "    if count==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
